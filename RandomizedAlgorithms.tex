\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\chapter{Randomized Algorithms}
\section{Quick Sort}
\subsection{Overview}
Quick sort is a prevalent sorting algorithm in practice. It is $O(n\log n)$ on average, and it works in place, i.e. extra memory need to carry out the sort is minimal, whereas for merge sort, we need at least $O(n)$ extra memory. The problem is the same as specified for merge sort. Here we assume that all items inside the array are distinct for simplicity. 

The key idea of merge sort is \textbf{partition the array around a pivot element}. Plenty of deliberation remains for the choice of the pivot element. For the moment we just assume that the first element is used. In a partition, the array is rearranged so that elements smaller than the pivot are put before the pivot, while elements larger than it are put after the pivot. The partition puts the pivot in the correct position. By recursively partition the two sub-arrays on both sides of the pivot, the whole array becomes sorted. As will be revealed later, a partition can be finished with $O(n)$ time and no extra memory. The skeleton of the algorithm is shown in Algorithm \ref{quicksortskeleton}.
\begin{algorithm}[ht]
\caption{Skeleton of Quick Sort}\label{quicksortskeleton}
\begin{algorithmic}[1]
\Input\Statex{Array $A$ with $n$ distinct elements in any order}
\Output\Statex{Array $A$ in sorted order}
\If{$A.len$ == 1}
\State{return}
\Else
\State{$p$ = ChoosePivot($A$)}
\State{Partition $A$ around $p$}
\State{Recursively sort 1st part(on left of $p$)}
\State{Recursively sort 2nd part(on right of $p$)}
\EndIf
\end{algorithmic}
\end{algorithm}
\subsection{Partition Subroutine}
If the in place requirement is thrown away, it is easy to come up with a partition algorithm using $O(n)$ time and $O(n)$ extra memory, as shown in Algorithm \ref{partitionextramemory}.
\begin{algorithm}[ht]
\caption{Partition with $O(n)$ Extra Memory}\label{partitionextramemory}
\begin{algorithmic}[1]
\Input\Statex{Array $A$ with $n(n>1)$ distinct elements in any order}
\Statex{Pivot element $p$, put at first position of $A$}
\Output\Statex{Array $A$ partitioned around $p$}
\State{Allocate $temp[n]$}
\State{$small = 1, big = n$}
\For{$i$ = 2 \textbf{to} $n$} 
\If{$A[i]>p$}
\State{$temp[big--] = A[i]$}
\Else\Comment{$A[i]<p$}
\State{$temp[small++] = A[i]$}
\EndIf
\EndFor
\State{Assert $small == big$}
\State{$temp[small] = p$}
\State{Copy $temp$ back to $A$}
\end{algorithmic}
\end{algorithm}

Now we try to implement an in-place partition algorithm that uses no extra memory. During the process, the array will be composed of 4 consecutive parts: the pivot $p$ at the first position, then elements smaller than $p$, elements larger than $p$ and finally elements remaining to be partitioned. Algorithm \ref{partitionnoextramemory} provides such an implementation. It completes the partition in one scan of the array, and uses no extra memory. $i$ is the index of the first element larger than $p$\footnote{If $A[2]$ is smaller than $p$, $i$ will not have the same meaning when it gets initialized to 2. But the algorithm is still correct. In such case, $i$ remains the same as $j$ until we encounter the first element larger than $p$, and $swap$ will not do anything because $k$ and $i$ are always the same when we do the comparison.}, while $j$ is the index of the first unpartitioned element. 
\begin{algorithm}[ht]
\caption{Partition with No Extra Memory}\label{partitionnoextramemory}
\begin{algorithmic}[1]
\Input\Statex{Array $A$ with $n(n>1)$ distinct elements in any order}
\Statex{Pivot element $p$, put at first position of $A$}
\Output\Statex{Array $A$ partitioned around $p$}
\State{$i = 2,\:j = 2$}
\For{$k$ = 2 \textbf{to} $n$} 
\If{$A[k]<p$}
\State{$swap(A[k],A[i++])$}
\EndIf
\State{$j++$}
\EndFor
\State{$swap(A[1],A[i-1])$}
\end{algorithmic}
\end{algorithm}
\subsection{Choice of Pivot}
The running time of quick sort depends on the choice of the pivot. The naive approach taken above to always choose the first element is not satisfactory. If the array is already sorted, each time the size of the problem is only reduced by 1, which is no better than selection sort, and the running time if $O(n^2)$. This is indeed the worst case of quick sort. On the contrary, if each time the pivot divides the array into sub-arrays of the same size, we will have the recurrence relation 
\begin{equation*}
T(n)=2\cdot T(n/2)+O(n),
\end{equation*}
and the running time will be $O(n\log n)$ according to the master method, which is the best case. 
\subsection{Quick Sort Theorem}
A good solution is to choose the pivot randomly at every recursive call. We will prove the quick sort theorem \ref{quicksorttheorem}. 
\begin{theorem}\label{quicksorttheorem}
For every input array $A$ of length $n$, the average running time of quick sort (with random pivots) is $O(n\log n)$.
\end{theorem}
The proof of the theorem involves some basic probability knowledge that won't be covered here. Let $\Omega$ represent the sample space of all possible sequences of pivots in quick sort. For any $\sigma\in\Omega$, let $C(\sigma)$ represent the number of comparisons between array elements made by quick sort. We have Lemma \ref{rtdominated}.
\begin{lemma}\label{rtdominated}
The running time of quick sort is dominated by comparisons, i.e. $\exists$ constant $c$ such that $\forall\sigma\in\Omega$, 
$$RT(\sigma)\leq c\cdot C(\sigma).$$
\end{lemma}
Lemma \ref{rtdominated} means that in order to prove the quick sort algorithm, all we need is to prove the expectation of $C(\sigma)$ is $O(n\log n)$. We cannot apply the master method here because in quick sort, the two sub-problems are unlikely to have the same size. 

For a fixed input array $A$, let $z_i$ represent its $i^{th}$ smallest element. Let $x_{ij}(\sigma)$ represent the number of comparisons between $z_i$ and $z_j$ made during quick sort with pivot sequence $\sigma.$ Whenever a comparison happens, one of the two elements being compared is the pivot. If $z_i$ and $z_j$ have been partitioned respectively into two sides of a pivot before neither is used as pivot, they will never be compared in the future. If $z_i$ is used as pivot and is compared against $z_j$, $z_i$ will be at its correct position at the end of the partition, and is excluded from any further comparisons. Thus $\forall i,j,\sigma$, it is clear that $x_{ij}(\sigma)$ is either 0 or 1. A random variable that can only take values 0 and 1, like $x_{ij}$ here, is called an \textbf{indicator}. 

$C(\sigma)$ can be expressed as the sum over all $x_{ij}$:
\begin{equation*}
C(\sigma)=\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^{n}x_{ij}(\sigma).
\end{equation*}
According to the \textbf{linearity of expectation}, an taking into account of the fact that $x_{ij}$ is an indicator, we have
\begin{equation*}
E[C]=\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^{n}E[x_{ij}]=\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^{n}P[x_{ij}=1].
\end{equation*}
Here we are actually applying a \textbf{decomposition approach} that goes for the analysis of average running time of a lot of randomized algorithms. 
\begin{enumerate}
\item Identify random variable $Y$ that we care about.
\item Express $Y$ as the sum of a series of indicators: $Y=\sum\limits_lx_l$.
\item Apply linearity of expectation: $E[Y]=\sum\limits_lE[x_l]=\sum\limits_lP[x_l=1]$.
\end{enumerate}
It can be proved that for any $i<j$,
$$P(x_{ij}=1)=\frac{2}{j-i+1}$$
\begin{proof}
Consider the set of elements 
$$S_{ij}=\{z_i,z_{i+1},\dots,z_{j-1},z_j\}.$$
As long as none of them is chosen as pivot, they will be passed to the same recursive call. If $z_i$ or $z_j$ is the first among them to be chosen as pivot, $x_i$ and $x_j$ will be compared. On the contrary, if any other element is chosen as pivot before $z_i$ and $z_j$, $z_i$ and $z_j$ will end up in different sub-arrays, and they can never be compared in the future. So $P(x_{ij}=1)$ is equal to the probability that $z_i$ or $z_j$ is the first element of $S_{ij}$ to be chosen as pivot, i.e. $\frac{2}{j-i+1}$.
\end{proof}
Then we have 
\begin{align*}
E[C]&=\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^{n}P[x_{ij}=1]=\sum\limits_{i=1}^{n-1}\sum\limits_{j=i+1}^{n}\frac{2}{j-i+1}\\
&=2\cdot\sum\limits_{i=1}^{n-1}\left(\frac{1}{2}+\frac{1}{3}+\dots+\frac{1}{n-i+1}\right)\\
&=2\cdot\left(\frac{n}{2}+\frac{n-1}{3}+\frac{n-2}{4}+\dots+\frac{1}{n}\right)\\
&\leq2n\sum\limits_{k=2}^n\frac{1}{k}\\
&\leq2n\int_1^n\frac{1}{x}dx=2n\ln n
\end{align*}
Thus the running time of quick sort with random pivots is $O(n\log n)$.
\section{Randomized Selection}
\subsection{RSelect Algorithm}
In this section we will discuss the selection problem.
\begin{description}
\item[Input]Array $A$ containing $n$ distinct numbers in arbitrary order, and number $i\in\{1,2,\dots,n\}$. 
\item[Output]The $i^{th}$ order statistic, i.e. $i^{th}$ smallest element of $A$.
\end{description}
Obviously, the $i^{th}$ order statistic can be found by sorting the array and then directly pick the $i^{th}$ element. The overall running time is at least $O(n\log n)$. A randomized approach similar to that of quick sort can reduce the average running time to $O(n)$, which is quite amazing because simply reading all elements of the array is already an $O(n)$ process. The algorithm is provided in Algorithm \ref{randomizedselection}. 
\begin{algorithm}[ht]
\caption{Randomized Selection}\label{randomizedselection}
\begin{algorithmic}[1]
\Input\Statex{Array $A$ with $n$ distinct elements in any order}
\Statex{Integer $i\in\{1,2,\dots,n\}$}
\Output\Statex{$i^{th}$ order statistic of $A$}
\Function{$RSelect$}{$A,i$}
\If{$A.len==1$}
\State{return $A[1]$}
\EndIf
\State{Randomly choose pivot $p$ from $A$}
\State{Partition $A$ around $p$}
\State{Let $j$ = new index of $p$, $A_1,A_2$ = 1st and 2nd part of $A$}
\If{$j==i$}
\State{return $p$}
\ElsIf{$j>i$}
\State{return $RSelect(A_1,i)$}\Comment{$A_1.len=j-1$}
\Else\State{return $RSelect(A_2,i-j)$}\Comment{$j<i.\:A_2.len=n-j$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
\subsection{Running Time}
$RSelect$ is also based on recursively partitioning the arrays, but at most one recursive call is needed in each iteration because the $i^{th}$ order statistic can only be on one side of the pivot. As with quick sort, the worse case happens when each time the smallest element is chosen as pivot. The worst running time is $O(n^2)$. The average running time is given by Theorem \ref{rselecttheorem}.
\begin{theorem}\label{rselecttheorem}
For any input array $A$ of length $n$, the average running time of $RSelect$ is $O(n)$.
\end{theorem}
In $RSelect$, the only workload outside of the recursive call is the partition, which is an $O(n)$ process. Another way to note its time complexity is that it uses fewer than $cn$ operations, in which $c$ is a constant. 
\begin{definition}
RSelect is said to be in phase $j$ if size of the current array is between $(3/4)^{j+1}n$ and $(3/4)^jn$.
\end{definition}
Let $x_j$ represent the number of recursive calls during phase $j$. Then we have
\begin{equation}\label{rtrselect}
RT(RSelect)\leq cn\sum\limits_jx_j\cdot\left(\frac{3}{4}\right)^j
\end{equation}
Now let's consider $E(x_j)$. If $RSelect$ chooses a pivot giving a 25\%-75\% or better partition, the execution is guaranteed to enter the next phase, because the length of the sub-array to examine will shrink by at least 25\%. The probability for such a partition to happen is obviously 50\%. The problem is analogous to the coin flipping problem. The expectation of $x_j$ is the same as the expectation of the number of flips needed to have the ``head'' side of the coin show up for the first time. The coin flipping problem is a geometric distribution problem, in which the expectation is $\frac{1}{p}=\frac{1}{1/2}=2$. Thus $E(x_j)=2$. Substitute this into \eqref{rtrselect}, we have
\begin{align*}
E(RT(RSelect))\leq cn\sum\limits_jE(x_j)\left(\frac{3}{4}\right)^j=2cn\sum\limits_j\left(\frac{3}{4}\right)^j\leq8cn.
\end{align*}
\section{Deterministic Selection}
In this section we will cover a deterministic selection algorithm that is also $O(n)$. It is not a good choice when compared against random selection because the constant for the big-O notation is larger, and it is not in place. Yet it is still a brilliant and interesting algorithm.

When partitioning an array, the ideal pivot is its median. In order to guarantee the efficiency of the algorithm, we need to find a good enough pivot. The key idea is to use the ``median of medians''. The array is broken into $n/5$ sub-arrays of length 5. The sub-arrays are sorted, their medians are put into a new array $C$, and we recursively compute the median of $C$. The final result is returned as the pivot. The algorithm is shown in Algorithm \ref{dselection}.
\begin{algorithm}[ht]
\caption{Deterministic Selection}\label{dselection}
\begin{algorithmic}[1]
\Input\Statex{Array $A$ with $n$ distinct elements in any order}
\Statex{Integer $i\in\{1,2,\dots,n\}$}
\Output\Statex{$i^{th}$ order statistic of $A$}
\Function{$DSelect$}{$A,i$}
\State{$n=A.len$}
\If{$n\leq 5$}
\State{sort $A$ and pick the $i^{th}$ element}
\EndIf 
\State{Break $A$ into $n/5$ sub-arrays of length 5 and sort each sub-array}\Comment{$O(n)$}
\State{Let array $C$ store the medians of the $n/5$ sub-arrays}\Comment{$O(n)$}
\State{$p=DSelect(C,n/10)$}\Comment{Recursively compute median. $T(n/5)$}
\State{Partition $A$ around $p$}\Comment{$O(n)$}
\State{Let $j$ = new index of $p$, $A_1,A_2$ = 1st and 2nd part of $A$}
\If{$j==i$}\State{return $p$}
\ElsIf{$j>i$}\State{return $DSelect(A_1,i)$}\Comment{$T(?)$ running time to be determined}\label{line1}
\Else\State{return $DSelect(A_2,i-j$)}\label{line2}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
Two recursive calls are made in Algorithm \ref{dselection}. Let $T(n)$ represent the running time of $DSelect$ on an array of length $n$, then there exists constant $c$ such that 
\begin{itemize}
\item $T(1)=1$
\item $T(n)\leq cn+T(n/5)+T(?)$
\end{itemize}
in which the running time of the second recursive call needs to be determined. 
\begin{lemma}
The 2nd recursive call (line \ref{line1} or \ref{line2}) is guaranteed to be $O(\frac{7}{10}n)$.
\end{lemma}
\begin{proof}
Let $k=n/5$, and let $x_i$ represent the $i^{th}$ smallest of the $k$ medians. Then the final pivot is $x_{k/2}$. Our goal is to prove that at least 30\% of the elements in the input array are smaller than the $x_{k/2}$, and at least 30\% are bigger than it. Let's put all elements of the array in a 2D grid as follow, with each sorted group as a column, and $x_i$ in ascending order. 
\begin{table}[H]
\centering
\begin{tabular}{ccccccc}
{\color{red}$\circ$}&{\color{red}$\circ$}&{\color{red}$\cdots$}&{\color{red}$\circ$}&$\cdots$&$\circ$&$\circ$
\\
{\color{red}$\circ$}&{\color{red}$\circ$}&{\color{red}$\cdots$}&{\color{red}$\circ$}&$\cdots$&$\circ$&$\circ$
\\
{\color{red}$x_1$}&{\color{red}$x_2$}&{\color{red}$\cdots$}&$x_{k/2}$&{\color{blue}$\cdots$}&{\color{blue}$x_{k-1}$}&{\color{blue}$x_k$}\\
$\circ$&$\circ$&$\cdots$&{\color{blue}$\circ$}&{\color{blue}$\cdots$}&{\color{blue}$\circ$}&{\color{blue}$\circ$}\\
$\circ$&$\circ$&$\cdots$&{\color{blue}$\circ$}&{\color{blue}$\cdots$}&{\color{blue}$\circ$}&{\color{blue}$\circ$}
\end{tabular}
\end{table}
Obviously, elements in red color are smaller than $x_{k/2}$, while elements in blue color are bigger than it. Both include $\frac{3}{5}\times\frac{1}{2}=30\%$ of the elements. Thus the size of the sub-array is guaranteed to shrink by at least 30\% in the 2nd recursive call, so its running time is $O(\frac{7}{10}n)$.
\end{proof}
\begin{theorem}
$\forall$ input array of size $n$, $DSelect$ runs in $O(n)$ time.
\end{theorem}
\begin{proof}
Now we have 
\begin{equation*}
T(n)\leq cn+T\left(\frac{n}{5}\right)+T\left(\frac{7n}{10}\right).
\end{equation*}
The master method is not applicable because the two sub-problems are not of the same size. We will prove $T(n)\leq 10cn$ by induction.

The base case($n=1$) is trivial. Suppose that $T(k)\leq 10ck$ holds for all $k<n$. Then we have 
\begin{align*}
T(n)\leq&cn+T\left(\frac{n}{5}\right)+T\left(\frac{7n}{10}\right)\\
\leq&cn+10c\cdot\frac{n}{5}+10c\cdot\frac{7n}{10}\\
=&10cn
\end{align*} 
Hence $T(n)$ is $O(n)$.
\end{proof}
\section{Lower Bound for Comparison-Based Sorting}
Up to now we have in our toolbox $O(n\log n)$ sorting algorithms and $O(n)$ selection algorithms. In this section we will prove that we cannot do better with comparison-based sorting.
\begin{definition}
In a comparison-based sorting, the array elements can only be accessed via comparisons. 
\end{definition}
Merge sort, heap sort, quick sort, selection sort are all examples of comparison-based sorting. Bucket sort, counting sort, radix sort are examples of non-comparison-based sorting. They require extra knowledge of the data to sort. 

The following theorem provides a lower bound for the running time of comparison-based sorting. 
\begin{theorem}
Every comparison-based sorting algorithm has worst-case running time $\Omega(n\log n)$.
\end{theorem}
\begin{proof}
Consider the action of a comparison-based sorting algorithm on an input composed of integers from 1 to $n$. There are totally $n!$ such inputs. Let $k$ be the minimum number of comparisons needed to address all these inputs. Each comparison has two possible results, thus $k$ comparisons have $2^k$ possible combinations of results. In order for all the $n!$ possible inputs to be sorted correctly, we must have
$$2^k\leq n!,$$
because otherwise there would be at least 2 inputs that appeared the same for the comparison-based algorithm, according to the pigeonhole principle\footnote{Also named drawer principle}. Hence we have 
$$k\leq\log(n!)<\log(n/2)^{n/2}=\frac{n}{2}\log\frac{n}{2},$$
which means that the running time of the algorithm is $\Omega(n\log n)$.


\end{proof}
\ifx\PREAMBLE\undefined
\end{document}
\fi