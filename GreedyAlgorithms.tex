\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\chapter{Greedy Algorithms}
In the field of algorithm design, there is no silver bullet suitable for all kinds of problems. We have covered divide-and-conquer paradigm and randomized algorithms. We will now introduce greedy algorithms, and in the next chapter we will dive into dynamic programming.

In a greedy algorithm, ``myopic'' decisions are made iteratively, in the hope that finally we will end up with a correct solution to the problem. Dijkstra's algorithm is actually a greedy algorithm. In contrast with divide-and-conquer algorithms, greedy algorithms have the following features:
\begin{itemize}
\item It is easy to propose multiple greedy algorithms for many problems.
\item It is relatively easy to analyze running times of greedy algorithms.
\item It is hard to establish the correctness of greedy algorithms. Proofs are usually ad-hoc. General approaches include induction (e.g. for Dijkstra) and ``exchange argument'', which will be covered later.
\item Most greedy algorithms are unfortunately incorrect.
\end{itemize}

A problem that can theoretically be solved by a greedy algorithm is the caching problem. Modern computers contain caches, sometimes a few layers. On a cache miss that happened at a page request, we have to evict something from the cache in order to make room for the newly requested page that is not in the cache. The choice of the item to evict influences greatly the number of cache misses. Some misses are inevitable, while others happen can be avoided by wise eviction choices. 

It can be proved via exchange argument that the ``furthest-in-future'' algorithm in the optimal solution to this problem. The algorithm always evicts the item that will take the longest time to be requested again. Although its correctness can be verified, this algorithm obviously cannot be implemented. Nonetheless it is useful as a guideline for practical algorithms like LRU(least-recently-used) algorithm, and it also serves as an idealized benchmark for caching algorithms.
\section{Scheduling}
\begin{description}
\item[Input]A set of $n$ jobs (e.g. processes) that have to use a shared resource (e.g. a processor) exclusively. Each job $j$ has a length $l_j$ and a weight $w_j$. 
\item[Output]An order to execute the jobs that minimizes the weighted sum of completion times $\sum\limits_{j=1}^{n}w_jC_j.$
\end{description}
Two simple cases of the problem are when the jobs have the same lengths / weights. If they have the same length, jobs with larger weights should be scheduled earlier, whereas if they are of the same weight, jobs with shorter lengths should go first. The scenarios might be extended to handle more general cases if we are able to resolve conflicts: what if $w_i>w_j$ and $l_i>l_j$? This can be achieved by assigning scores to jobs, and scheduling jobs with higher scores in front. The score has to increase with weight and decrease with length. Two intuitive choices are 
\begin{itemize}
\item $w_j-l_j$
\item $w_j/l_j$
\end{itemize}
A simple 2-job case with $l_1=5,\allowbreak w_1=3$ and $l_2=2,\allowbreak w_2=1$ rules the first option out. We will try to prove the correctness of the second option, whose correctness is absolutely not trivial.
\begin{proof}
First we assume that all jobs have distinct scores, i.e. $w_i/l_i\neq w_j/l_j$ for $i\neq j$. This case can be addressed via contradiction.

The $n$ jobs can be renamed so that $\frac{w_1}{l_1}>\frac{w_2}{l_2}>\dots>\frac{w_n}{l_n}$. According to the rule above, the optimal order should be $1,2,\dots,n$. Suppose that there exists an order superior to this one. In this order, there must exist at least one pair of consecutive jobs $i,j$ such that $i>j$ but $i$ is behind $j$. If we exchange $i,j$, the completion time of $i$ will decrease by $l_j$, whilst that of $j$ will increase by $l_i$. In total, the weighted sum of completion times decreases by
$$w_il_j-w_jl_i.$$
Since $i>j$, we must have $\frac{w_i}{l_i}>\frac{w_j}{l_j}$, thus $w_il_j>w_jl_i$. In conclusion, we have obtained a better order than the one supposed to be the optimal, which negates the initial assumption.

With similar argument, the correctness of the algorithm can be verified for the general case with possible ties in score. In an arbitrary order of the jobs, a consecutive pair $(i,j)$ with $i>j$ and $i$ behind $j$ can be called an inversion\footnote{The definition of inversion here is different from that in Algorithm \ref{inversioncounting}.} The number of inversions is at most $\frac{n(n-1)}{2}$, and the only order without any inversion is $1,2,\dots,n$. Since we have $w_il_j\geq w_jl_i$ for $i>j$, exchanging an inversion is guaranteed not to increase the weighted sum of completion times. Each exchange decreases the number of inversions strictly by one. Thus after at most $\frac{n(n-1)}{2}$ exchanges, we must arrive at the order $1,2,\dots,n$ with no increase of the weighted sum. Therefore $1,2,\dots,n$ is at least as good as any other order in terms of weighted sum of completion times, making it a guaranteed optimal solution to the scheduling problem.
\end{proof}
\section{Minimum Spanning Tree}
Minimum spanning tree is a problem to which there exist a bunch of correct and fast greedy solutions. We will discuss two of them: Prim's algorithm and Kruskal's algorithm.
\begin{description}
\item[Input]An undirected graph $G(V,E)$ with a possibly negative cost $c_e$ for each $e\in E$. 
\item[Output]A minimum cost tree $T\subseteq E$ that spans all vertices, i.e. connected subgraph $(V,T)$ that contains no cycles with minimum sum of edge costs.
\end{description}
In order to facilitate the discussion, we assume that graph $G$ is connected, and that edge costs are distinct, although Prim and Kruskal remain correct for ties in edge costs.
\subsection{Prims' Algorithm}
Prim's MST algorithm is shown in Algorithm \ref{primmst}. 
\begin{algorithm}[ht]
\caption{Prim's MST Algorithm}\label{primmst}
\begin{algorithmic}[1]
\Input{Undirected graph $G(V,E)$ with distinct cost $c_e$ for all $e\in E$.}
\Output{MST of $G$}
\State{Initialize $X=\{s\}$, $s\in V$ chosen arbitrarily}
\State{Initialize $T=\emptyset$}
\While{$X\neq V$}
\State{Let $e=(u,v)$ be the cheapest edge of $G$ with $u\in X, v\notin X$}
\State{Add $e$ to $T$}
\State{Add $v$ to $X$}
\EndWhile
\end{algorithmic}
\end{algorithm}
\subsubsection{Correctness}
We will prove its correctness in two steps. First, we verify that it does compute a spanning tree $T^*$. Then we prove that $T^*$ is an MST. 
\begin{lemma}\textbf{(Empty Cut Lemma)}\label{emptycutlemma}
A graph is not connected if and only if $\exists$ cut $(A,B)$ with no crossing edges.
\end{lemma}
\begin{proof}
$(\Leftarrow)$The proof is trivial. Just take vertex $u\in A$ and $v\in B$. There cannot exist any edges between $u,v$, thus the graph is not connected.

$(\Rightarrow)$Suppose there exists no path between $u,v$. Take $A=\{$All vertices reachable from $u\}$, i.e. the connected component of $u$, $B=\{$All other vertices$\}$, i.e. other connected components. Then there exists no crossing edges of the cut $(A,B)$.
\end{proof}
\begin{lemma}\label{doublecrossinglemma}
\textbf{(Double Crossing Lemma)}
Suppose the cycle $C\subseteq E$ has an edge crossing the cut $(A,B)$, then there must exist some other edge $e\in C$ that crosses the same cut.
\end{lemma}
\begin{corollary}
\textbf{(Lonely Cut Corollary)}\label{lonelycutcorollary}
If $e$ is the only edge crossing a cut $(A,B)$, then it is not contained in any cycle.
\end{corollary}
With the lemmas and the corollaries above, we can prove that Prim's algorithm outputs a spanning tree.
\begin{proof}
It can be proved by induction that Prim's algorithm maintains the invariant that $T$ spans $X$. The proof of connectivity is trivial. No cycle can be created in $T$ because each time an edge $e$ is added into $T$, it becomes the only crossing edge of the cut $(X,\{v\})$ of $T$, and therefore cannot be contained in a cycle according to Corollary \ref{lonelycutcorollary}. 

The algorithm cannot get stuck when $X\neq V$, because otherwise the cut $(X,V-X)$ must be empty, and according to Lemma \ref{emptycutlemma}, the graph would be disconnected. 

As a conclusion, Prim's algorithm is guaranteed to output a spanning tree of the original graph.
\end{proof}
The second part of the proof is based on the cut property.
\begin{theorem}\label{cutproperty}
\textbf{(Cut Property)}
Consider an edge $e$ of graph $G$. If $\exists$ cut $(A,B)$ such that $e$ is the cheapest crossing edge of the cut, then $e$ belongs to the\footnote{We use ``the'' rather than ``a'' because the MST is unique if edge costs are distinct.} MST of $G$.
\end{theorem}
\begin{proof}
The cut property can be proved by exchange argument.

Suppose there is an edge $e$ that is the cheapest crossing edge of a cut $(A,B)$, yet $e$ is not in the MST $T^*$. As shown in Figure \ref{proofcutproperty}, in which all blue edges form the minimum spanning tree $T^*$, and the minimum crossing edge $e$ of $(A,B)$ is not contained in $T^*$.
\begin{figure}[ht]
\centering
\begin{tikzpicture}
\tikzstyle{nodestyle} = [circle, draw=blue, fill=blue!20!]
\tikzstyle{usualedge} = [blue, very thick]
\tikzstyle{emphedge} = [red, very thick]
\tikzstyle{label} = [midway, above]
\node (A) at (-2,0.5) {A};
\node (B) at (2,0.5) {B};
\node[nodestyle](1) at (-2,0){1};
\node[nodestyle](2) at (-2,-1){2};
\node[nodestyle](3) at (-2,-2){3};
\node[nodestyle](4) at (2,0){4};
\node[nodestyle](5) at (2,-1){5};
\node[nodestyle](6) at (2,-2){6};
\draw[usualedge] (1) -- (2);
\draw[usualedge] (2) -- (3);
\draw[usualedge] (1) -- (4) node[label] {f};
\draw[emphedge] (2) -- (5) node[label] {e};
\draw[usualedge] (3) -- (6) node[label] {e'};
\draw[usualedge] (5) -- (6);
\draw (2) ellipse (1cm and 2cm);
\draw (5) ellipse (1cm and 2cm);
\end{tikzpicture}
\caption{Proof of cut property}\label{proofcutproperty}
\end{figure}

We cannot exchange $e$ with a random crossing edge of the cut $(A,B)$. In this example, if we exchange $e$ with $f$, we no longer have a spanning tree. Rather, if $e$ is exchanged with $e'$, we obtain a spanning tree with smaller cost than $T^*$. Our task is to prove that such an edge $e'$ always exists. 

Since $T^*$ is a spanning tree, $T^*\cup\{e\}$ must contains a cycle that includes $e$. According to Lemma \ref{doublecrossinglemma}, there must exist another edge $e'$ that crosses the cut $(A,B)$. According to the assumption, $e'$ must be more expensive than $e$. By substituting $e'$ with $e$ in $T^*$, we obtain a spanning tree $(T^*-\{e'\})\cup\{e\}$ with smaller cost than $T^*$, which contradicts with our assumption that $T^*$ is the MST.
\end{proof}

According to the cut property, each edge selected in Prim's algorithm is guaranteed to be part of the MST. Since we obtain a spanning tree in the end, it must be the MST. 
\subsubsection{Implementation}
A brute-force implementation of Prim's algorithm has $O(nm)$ running time. In each iteration, all edges going out of $X$ needs to be scanned and the cheapest among them is chosen. The scanning process is $O(m)$, and there are totally $n$ iterations, thus the overall running time is $O(nm)$. This may not seem fast, but considering the fact than there exist $2^m$ possible sub-graphs among which the MST has to be selected, it already provides plenty of performance amelioration.

Using heap can make Prim's algorithm run even faster. A straightforward idea is to use the heap to store crossing edges of the cut $(X,V-X)$, as shown in Algorithm \ref{primmstheap1}.
\begin{algorithm}[ht]
\caption{Prim's MST Algorithm, Heap Implementation 1}\label{primmstheap1}
\begin{algorithmic}[1]
\Input{Undirected graph $G(V,E)$ with distinct cost $c_e$ for all $e\in E$.}
\Output{MST of $G$}
\State{Initialize an empty heap $H$ and an empty set of nodes $X$.}
\State{Randomly select a node $s$ and add it to $X$.}
\State{Insert all edges $(s,v)$ into the heap.}
\While{$X\neq V$}
\State{Add edge $e=H.extractMin()$ to $T$.}
\State{Add the node $n$ of $e$ not in $X$ to X.}
\For{each edge $(n,v)\in E$}
\If{$v\in V-X$}
\State{Insert $(n,v)$ into $H$.}
\ElsIf{$(n,v)\in H$}
\State{Delete $(n,v)$ from $H$.}
\EndIf
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}
Each edge can be inserted into and deleted from the heap at most once respectively, thus the overall running time is $O(m\log m)$. 

The heap can also be used to store vertices in $V-X$, with the key of node $v$ being the cost of the cheapest edge $(u,v)$ with $u\in X$, or $\infty$ if such edge does not exist. The implementation is shown in Algorithm \ref{primmstheap2}.
\begin{algorithm}[ht]
\caption{Prim's MST Algorithm, Heap Implementation 2}\label{primmstheap2}
\begin{algorithmic}[1]
\Input{Undirected graph $G(V,E)$ with distinct cost $c_e$ for all $e\in E$}
\Output{MST of $G$}
\State{Initialize heap $H$ with all nodes in $V$. Obviously all nodes have key $\infty$.}
\State{Initialize empty set of nodes $X$.}
\While{$H$ is not empty}
\State{Add node $v=H.extractMin()$ to $X$.}
\State{Add the edge associated with $v$ to $T$ if it exists.}
\For{each edge $(v,w)\in E$}
\If{$w\in V-X$ \textbf{and} $cost(v,w)<key[w]$}
\State{Change $key[w]$ to $cost(v,w)$.}
\State{Change the edge associated with $w$ to $(v,w)$.}
\State{Bubble up the changed node in $H$.}
\EndIf
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}

In total the minimum of the heap is extracted $n$ times. Each edge is possible to trigger a bubble up. Since the graph is connected, we have $m\geq n-1$. Therefore, the overall running time of Algorithm \ref{primmstheap2} is $O(m\log n)$.
\subsection{Kruskal's Algorithm}
Kruskal's algorithm is another efficient greedy algorithm that solves the MST problem.
\begin{algorithm}[ht]
\caption{Kruskal's MST Algorithm}\label{kruskalmst}
\begin{algorithmic}[1]
\Input{Undirected connected graph $G(V,E)$ with distinct cost $c_e$ for all $e\in E$}
\Output{$T$: MST of $G$}
\State{Sort all edges in order of increasing cost.}
\State{Rename the edges so that $c_1<c_2<\dots<c_m$.}
\State{Initialize set of edges $T=\emptyset$.}
\For{$i=1$ \textbf{to} $m$}\Comment{Can terminate when $T$ already contains all nodes.}
\If{$T\cup\{i\}$ has no cycle}
\State{Add $i$ to $T$}
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
\subsubsection{Correctness}
We will prove the correctness of Kruskal's algorithm in a similar way to that of Prim's algorithm, i.e. first we verify that it outputs a spanning tree, then we demonstrate that this tree is the MST.

Let $T^*$ be the output of Kruskal's algorithm on a graph $G$. It can be proved that $T^*$ contains all nodes of $G$. For a random node $n$, consider the cut $(\{n\},V-\{n\})$ of $G$. Since $G$ is connected, there must exist at least one edge crossing this cut. Let $e$ represent the cheapest among them. When Kruskal's algorithm examines $e$, it is guaranteed to include it into $T$, because $e$ is at the moment the only crossing edge of the cut inside $T$ and thus cannot induce any cycle according to the lonely cut corollary (\ref{lonelycutcorollary}). Therefore $n$ is contained in $T^*$. 

It is enforced by the algorithm that $T^*$ contains no cycle. In order to prove that $T^*$ is a spanning tree of $G$, we need to verify that $T^*$ is connected, which according to empty cut lemma (\ref{emptycutlemma}) is equivalent to the fact that it crosses every cut of $G$. For any cut $(A,B)$ of $G$, the same conclusion as that proved for $(\{n\},V-\{n\})$ above still holds, i.e. $T^*$ must contain the cheapest crossing edge of this cut. Therefore $T^*$ must cross any cut, and we have succeeded in proving that Kruskal's algorithm outputs a spanning tree of $G$.   

In order to prove that $T^*$ is the MST of $G$, we just have to prove that each edge of $T^*$ satisfies the cut property (\ref{cutproperty}). 

Consider an iteration in Kruskal's algorithm that adds edge $(u,v)$ into $T$, as illustrated in Figure \ref{proofkruskal}. 
\begin{figure}[ht]
\centering
\begin{tikzpicture}
\tikzstyle{majornode} = [circle, draw=blue, fill=blue!20!]
\tikzstyle{minornode} = [circle, draw, minimum size=0.3cm]
\tikzstyle{minoredge} = [thick, black]
\tikzstyle{majoredge} = [very thick, red]
\node[majornode] (u) at (-2, -1) {u};
\node[majornode] (v) at (1, -3) {v};
\node[minornode] (u1) at (-2.5,-1.5){};
\node[minornode] (u2) at (-3,-2){};
\node[minornode] (u3) at (-2.2,-2.5){};
\node[minornode] (u4) at (-2,-1.8){};
\node[minornode] (v1) at (2.3,-1.5){};
\node[minornode] (v2) at (2.4,-2.2){};
\node[minornode] (v3) at (0.8,-2){};
\node[minornode] (v4) at (1.2,-1.4){};
\node[minornode] (n1) at (0.5, -0.8) {};
\node[minornode] (n2) at (0, -1.8) {};
\node[minornode] (n3) at (-0.2, -0.5) {};
\node[minornode] (n4) at (2, -0.3) {};
\draw[minoredge] (n1) -- (n2);
\draw[minoredge] (u) -- (u1);
\draw[minoredge] (u1) -- (u2);
\draw[minoredge] (u1) -- (u3);
\draw[minoredge] (u3) -- (u4);
\draw[minoredge] (v) -- (v1);
\draw[minoredge] (v1) -- (v2);
\draw[minoredge] (v1) -- (v3);
\draw[minoredge] (v3) -- (v4);
\draw[majoredge] (u) -- (v);
\draw[blue] (u1) ellipse (1cm and 1.5cm);
\draw[blue] (v4) ellipse (2.3cm and 2cm);
\node[blue] (A) at (-2.5, -0.5) {A};
\node[blue] (B) at (1, 0) {B};
\end{tikzpicture}
\caption{Proof of Kruskal's Algorithm}\label{proofkruskal}
\end{figure}

Since $(u,v)$ does not induce any cycle, there cannot exist any path from $u$ to $v$ in $T$. Take $A=$\{connected component of $u$ in $T$\}, $B=$\{All other nodes of $G$\}, then $(u,v)$ is the first crossing edge of this cut examined by Kruskal's algorithm (if there existed any other, the associated node in $B$ would be connected to $u$, and should have been contained in $A$), thus is also the cheapest crossing edge of the cut, which means that it satisfies the cut property. Hence, all edges in $T^*$ satisfy the cut property. $T^*$ is 
therefore the MST of $G$.
\subsubsection{Implementation}
Sorting the edges takes $O(m\log m)$ time. Assume that there exists no parallel edge, then $m=O(n^2)$, so the running time of the sorting is $O(m\log n)$. Totally there are $O(m)$ iterations. In each iteration, the existence of cycle in the intermediate sub-graph $T$, which contains at most $n-1$ edges, can be checked via DFS or BFS in $O(n)$ time\footnote{A JAVA implementation using DFS can be found at \href{http://algs4.cs.princeton.edu/code/edu/princeton/cs/algs4/Cycle.java.html}{\path{http://algs4.cs.princeton.edu/code/edu/princeton/cs/algs4/Cycle.java.html}}}. The total running time is therefore $O(m\log n) + O(mn)=O(mn)$. 

If we can find a data structure that allows cycle check in constant time, sorting the edges will be come the bottleneck and the running time of Kruskal's algorithm will become $O(m\log n)$. Union-Find is a data structure that makes this feasible.

Union-Find data structure maintains the partition of a set of objects. It supports two essential operations:
\begin{description}
\item[$find(X)$]Returns the name of the group to which $X$ belongs.
\item[$union(C_i, C_j)$]Merge groups $C_i,C_j$ into one group.
\end{description} 
In the problem of cycle check, the objects are the nodes, and they are partitioned into different connected components of $(V,T)$. When a new edge $(u,v)$ is added to $T$, if $u,v$ are inside the same group, i.e. $find(u)=find(v)$, then $(u,v)$ introduces a cycle. 

In order that the cycle check operation is $O(1)$, the union-find here needs to carry out $find(u)$ in constant time. This can be achieved by maintaining a linked structure per connected component of $(V,T)$. Each component has an arbitrary leader node to which all nodes in the same connected component point. The leader represents the name of the connected component. $find(u)$ returns the leader in $O(1)$ time, thus cycle check can be finished in $O(1)$ time. 

When a new edge $(u,v)$ is added to $T$, connected components of $u,v$ should merge into one. When two components merge, we have the smaller one inherit the leader of the larger one. At most $O(n)$ pointer updates are required for a merge. Nonetheless, the leader of each node can be updated at most $O(\log n)$ times, because each merge in which the leader gets updated at least doubles the size of the component to which the node belongs. 

It takes $O(m\log n)$ time to sort the edges, $O(m)$ time to check cycles, and $O(n\log n)$ to update leaders during merges. In total, Kruskal's algorithm has $O(m\log n)$ running time, which matches with Prim's algorithm.
\subsection{State-of-the-Art and Open Questions}
Prim's and Kruskal's algorithms find the MST in $O(m\log n)$ time. But can we do better? The answer is yes. An $O(m)$ randomized algorithm and an $O(m\alpha(n))$ deterministic algorithm have been found. The optimal deterministic algorithm has also been found, yet its running time is still unclear. So it remains an open question whether or not there exists an $O(m)$ deterministic solution to the MST problem.
\subsection{Clustering}
Clustering is also called unsupervised learning, which is an important problem in machine learning. The target is to classify $n$ given ``points'' into $k$ coherent groups. These points are not necessarily geometrical points. They can be web pages, images, genome fragments, etc, but it is required that a similarity measure, i.e. a distance function $d(p,q)$ should be defined for the points. $d(p,q)$ should be symmetric. 
\begin{definition}
The spacing of a $k$-clustering is defined as $$\min\limits_{p\in C_i,q\in C_j,i\neq j}d(p,q),$$ in which $C_i,C_j$ are two different clusters.
\end{definition}
\begin{description}
\item[Input]$n$ points with distance function $d(p,q)$.
\item[Output]$k$-clustering with maximum spacing.
\end{description}
A greed algorithm is provided in Algorithm \ref{greedyclustering}. It is quite analogous to Kruskal's MST algorithm: the points are the vertices, the distances are the edge costs, and each pair of point is an edge. This approach to solving the clustering problem is called single link clustering.
\begin{algorithm}[ht]
\caption{Greedy Clustering Algorithm}\label{greedyclustering}
\begin{algorithmic}[1]
\State{Initialize each point as a cluster.}
\Repeat
\State{Choose $p\in C_i,q\in C_j,i\neq j$ such that $d(p,q)$ is minimized.}
\State{Merge $C_i,C_j$ into one cluster.}
\Until{There are $k$ clusters left.}
\end{algorithmic}
\end{algorithm}

To prove the correctness of Algorithm \ref{greedyclustering}, we again turn to exchange argument.
\begin{proof}
Let $C_1,C_2,\dots,C_k$ be the $k$-clustering found by the greedy algorithm with spacing $S$, and $\hat{C_1},\hat{C_2},\dots,\hat{C_k}$ be another arbitrary $k$-clustering with spacing $\hat{S}$. We need to verify that $\hat{S}\leq S$.

If $\hat{C_i}$'s are just a renaming of $C_i$'s, then they obviously have the same spacing. Otherwise, there must exist at least one pair of points $p,q$ such that $p,q\in C_i$ but $p\in \hat{C_i}, q\in\hat{C_j}, i\neq j.$

If $p,q$ were chosen in some iteration of Algorithm \ref{greedyclustering}, i.e. they were ``directly merged'', then we must have $d(p,q)\leq S$. Since $\hat{S}\leq d(p,q)$, we've already proved $\hat{S}\leq S$.

If $p,q$ were not chosen in any iteration of the greedy algorithm, i.e. they were ``indirectly merged'', then let's consider the path from $p$ to $q$ inside $C_i$ formed by all direct greedy merges. Because $p\in \hat{C_i}, q\in\hat{C_j}, i\neq j,$ we are certainly able to find two consecutive points $p',q'$ on this path such that $p'\in C_i, q'\in C_j$. Then the problem is reduced to the case above, and we have $\hat{S}\leq d(p',q')\leq S$.   
\end{proof}
\section{Union Find}
\section{Huffman's Code}
\subsection{Problem Definition}
Find the best prefix-free encoding for a given set of character frequencies. 
$L(T)=\sum\limits_{i\in\Sigma}p_id_i$
\begin{description}
\item[Input]Probability $p_i$ for each character $i\in\Sigma$.
\item[Output]A binary tree $T$ that minimizes $L(T).$
\end{description}

\subsection{Greedy Algorithm}
\begin{algorithm}[ht]
\caption{Huffman's Algorithm}\label{huffman}
\begin{algorithmic}[1]
\If{$\lvert\Sigma\rvert=2$} 
\State{Return a tree with the two characters in $\Sigma$ as children.}
\EndIf
\State{Let $a,b$ be the two characters in $\Sigma$ that have the smallest frequencies.}
\State{Let $\Sigma'=\Sigma-\{a,b\}+\{ab\}$, and $p_{ab}=p_a+p_b$.}
\State{Recursively compute $T'$ for $\Sigma'$.}
\State{Expand $T'$ to a tree $T$ by splitting leaf $ab$ into two leaves $a,b$.}
\end{algorithmic}
\end{algorithm}
As an example, consider the following alphabet.
\begin{center}
\begin{tabular}{ccccccc}
character & A & B & C & D & E & F\\
weight    & 3 & 2 & 6 & 8 & 2 & 6
\end{tabular}
\end{center}
Running Huffman's algorithm will give us the result 
\begin{center}
\begin{tikzpicture}[
level 1/.style={sibling distance=4cm, level distance=1.5cm},
level 2/.style={sibling distance=2cm, level distance=1.5cm},
level 3/.style={sibling distance=2cm, level distance=1.5cm},
level 4/.style={sibling distance=2cm, level distance=1.5cm},
]
\tikzstyle{node} = [circle, draw, minimum size=0.5cm]
\tikzstyle{leafnode} = [circle, draw, minimum size=0.5cm, red]
\node[node] (1) at (0,0) {27}
	child { node[node] {15}
		child { node[node] {7}
			child { node[leafnode] {A(3)} edge from parent node[left]{0}}
			child { node[node] {4}
				child { node[leafnode] {B(2)} edge from parent node[left]{0}}
				child { node[leafnode] {E(2)} edge from parent node[right]{1}}
				edge from parent node[right]{1}
				}
			edge from parent node[left]{0}}
		child { node[leafnode] {D(8)} edge from parent node[right]{1}}
		edge from parent node[left]{0}}
	child { node[node] {12}
		child { node[leafnode] {C(6)} edge from parent node[left]{0}}
		child { node[leafnode] {F(6)} edge from parent node[right]{1}}
		edge from parent node[right]{1}};
\end{tikzpicture}
\end{center}
Finally we have the Huffman code:
\begin{center}
\begin{tabular}{ccccccc}
character & A & B & C & D & E & F\\
code    & 000 & 0010 & 10 & 01 & 0011 & 11
\end{tabular}
\end{center}
\subsection{Proof of Correctness}
We will prove the correctness of Huffman's algorithm by induction on $n=\lvert\Sigma\rvert>2$.

The base case is trivial: for an alphabet containing 2 characters, 1 bit is enough for the encoding. 
\ifx\PREAMBLE\undefined
\end{document}
\fi