\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\chapter{Dynamic Programming}
In this chapter we will introduce the last algorithm design paradigm: dynamic programming.
\section{Max-weight Independent Sets}
Our first example of dynamic programming is a relatively simple graph problem.
\begin{description}
\item[Input]A path graph $G(V,E)$ with non-negative weights on vertices.
\item[Output]An independent set, i.e. a subset of $V$ in which no vertices are adjacent, of maximum total weight.
\end{description}
\begin{center}
\begin{tikzpicture}
\tikzstyle{chosen} = [fill=red!20!, circle, draw=red]
\tikzstyle{normal} = [draw, circle]
\node[normal] (0) at (-4,0) {1};
\node[chosen] (1) at (-2,0) {4};
\node[normal] (2) at (0,0) 	{5};
\node[chosen] (3) at (2,0)	{4};
\draw (0) -- (1) -- (2) --(3);
\end{tikzpicture}
\end{center}
In the example above, the WIS is obviously the two red nodes. Generally, a brute-force approach takes exponential time. An intuitive greedy algorithm does not guarantee a correct answer: it is actually wrong for the simple example above.  The divide-and-conquer paradigm cannot be applied because there is no natural correct way to combine solutions to the two sub-problems. This is when dynamic programming comes to our rescue.

Let's consider the structure of an optimal solution in terms of its relationship with solutions to smaller problems. Let $S\subseteq V$ be a max-weight independent set (IS) of $G$, $v_n$ be the last vertex of the path and $v_{n-1}$ be the last but one vertex. Denote $G$ with $v_n$ deleted as $G'$, and $G$ with $v_n,v_{n-1}$ deleted as $G''$.  
\begin{itemize}
\item If $v_n\notin S$, then $S$ must also be a max-weight IS of $G'$, which can be proved easily by contradiction.
\item If $v_n\in S$, then $v_{n-1}\notin S$. It can be proved easily by contradiction that $S-\{v_n\}$ is a max-weight IS of $G''$. 
\end{itemize}
Therefore, a max-weight IS of $G$ is either a max-weight IS of $G'$, or a max-weight IS of $G''$ + $v_n$. The same reasoning holds for smaller problems, which induces a correct recursive algorithm:
\begin{enumerate}
\item Recursively compute $S_1$ = max-weight IS of $G'$.
\item Recursively compute $S_2$ = max-weight IS of $G''$.
\item Return $S_1$ or $S_2\cup\{v_n\}$, whichever is better.
\end{enumerate}
The correctness of the algorithm can be verified by induction. However it takes exponential time because it is per se a variant of the brute-force algorithm.
\begin{center}
\begin{tikzpicture}[
level 1/.style={sibling distance=6cm, level distance=1.5cm},
level 2/.style={sibling distance=3cm, level distance=1.5cm},
level 3/.style={sibling distance=2cm, level distance=1.5cm}]
level 4/.style={sibling distance=1cm, level distance=1.5cm}]
\tikzstyle{node} = [draw, circle,minimum size=0.8cm]
\node[node] (0) at (0,0) {n}
child {node[node]{n-1}
	child {node[node]{n-2}
		child {node[node]{n-3}}
		child {node[node]{n-4}}
		}
	child {node[node]{n-3}
		child {node[node]{n-4}}
		child {node[node]{n-5}}
		}
	}
child {node[node]{n-2}
	child {node[node]{n-3}
		child {node[node]{n-4}}
		child {node[node]{n-5}}
		}
	child {node[node]{n-4}
		child {node[node]{n-5}}
		child {node[node]{n-6}}
		}
	};
\end{tikzpicture}
\end{center}
As shown above, each sub-problem is calculated multiple times. The number of distinct sub-problems is actually $O(n)$. If we can reformulate the recursive algorithm into a bottom-up iterative algorithm, and cache the solution to a sub-problem the first time it is solved, the problem can be solved in linear time, as shown in Algorithm \ref{maxweightis}.
\begin{algorithm}[ht]
\caption{Max-weight Independent Set(DP)}\label{maxweightis}
\begin{algorithmic}[1]
\Input{Path graph $G=(V,E)$ with non-negative weight $w_i$ for each vertex $v_i$. Sub graph composed of the first $i$ vertices is denoted by $G_i$.}
\Output{Array $A$ with $A[i]=$ total weight of max-weight IS of $G_i$.}
\State{$A[0]=0,A[1]=w_1$.}
\For{$i = 2,3,\dots,n$}
\State{$A[i]=\max\{A[i-1], A[i-2]+w_i\}$}
\EndFor
\end{algorithmic}
\end{algorithm}

Algorithm \ref{maxweightis} only outputs the total weight of the max-weight IS of $G$. The IS itself can be reconstructed according to array $A$, as shown in Algorithm \ref{reconstructionis}. The running time is also $O(n)$.

\begin{algorithm}[ht]
\caption{Reconstruction of Max-weight Independent Set}\label{reconstructionis}
\begin{algorithmic}[1]
\Input{Array $A$ computed in Algorithm \ref{maxweightis}.}
\Output{The max-weight IS $S$ of path graph $G$.}
\State{Initialize $S=\emptyset$, $i=n$.}
\While{$i\geq 2$}
\If{$A[i-1]<A[i-2]+w_i$}
\State{Add $v_i$ to $S$}
\State{$i = i - 2$}
\Else
\State{$i = i - 1$}
\EndIf
\EndWhile
\If{$v_2\notin S$}
\State{Add $v_1$ to $S$}
\EndIf
\end{algorithmic}
\end{algorithm}
\section{Principles of DP}
After a concrete example, let's introduce a few general principles of dynamic programming. Typical DP problems  share the following traits:
\begin{enumerate}
\item It is easy to identify a small number of sub-problems. In the max-weight IS problem, the sub-problems are the max-weight IS of $G_i$ for $i=0,1,\dots,n$. The number of sub-problems is not necessarily linear, but it has to be reasonably small.
\item Given solutions to smaller sub-problems, larger sub-problems can be solved quickly can correctly. This is usually expressed as a recursive relation, for example $A[i]=\max\{A[i-1],A[i-2]+w_i\}$ in the max-weight IS problem.
\item The final solution can be computed quickly after solving all sub-problems. Usually it's just the answer to the largest sub-problem.
\end{enumerate}
\section{Knapsack Problem}
\begin{description}
\item[Input]$n$ items with non-negative value $v_i$ and non-negative integral size $w_i$ for item $i$. Capacity $W$, which is a non-negative integer.
\item[Output]A subset $S\subseteq\{1,2,\dots,n\}$ that maximizes $\sum\limits_{i\in S}v_i$ subject to the condition $\sum\limits_{i\in S}w_i\leq W$.
\end{description}
Again let's consider different situations for item $n$. If $n\notin S$, then $S$ must also be the optimal solution for the first $n-1$ items and capacity $W$. On the contrary, if $n\in S$, then $S-\{n\}$ must be the optimal solution for the first $n-1$ items and capacity $W-w_n$. Let $V_{i,x}$ represent the value of the best solution for the first $i$ items and capacity $x$, then recursively we have 
$$V_{i,x}=\max\{V_{i-1,x},V_{i-1,x-w_i}+v_i\}.$$
The sub-problems have been identified up to now: for each $i,x$ combination, there is a sub-problem. A DP algorithm is show in Algorithm \ref{knapsack}. The running time is obviously $O(nW)$.

\begin{algorithm}[ht]
\caption{Knapsack Problem(DP)}\label{knapsack}
\begin{algorithmic}[1]
\Input{$n$ items as stated above.}
\Input{$(n+1)\times(W+1)$ 2-D array $A$ with $A[i][x]=V_{i,x}$.}
\State{Initialize $A[0][x]=0$ for $x=0,1,\dots,W$.}
\For{$i=1,2,\dots,n$}
\For{$x=0,1,\dots,W$}
\If{$x\geq w_i$}
\State{$A[i][x]=\max\{A[i-1][x],A[i-1][x-w_i]+v_i\}$}
\Else\State{$A[i][x]=A[i-1][x]$}
\EndIf\EndFor\EndFor
\end{algorithmic}
\end{algorithm}

Array $A$ records the maximized sum of value for all sub-problems. The solution to the problem, i.e. the subset $S$, can be reconstructed according to $A$, as shown in Algorithm \ref{knapsackreconstruction}. Note that the running time is only $O(n)$.

\begin{algorithm}[ht]
\caption{Knapsack Reconstruction}\label{knapsackreconstruction}
\begin{algorithmic}[1]
\Input{Array $A$ computed in Algorithm \ref{knapsack}.}
\Output{Solution $S$ to the knapsack problem.}
\State{Initialize $S=\emptyset$, $i=n,x=W$.}
\While{$i\geq 1$}
\If{$A[i][x]\neq A[i-1][x]$}
\State{Add $i$ to $S$}
\State{$x=x-w_i$}
\EndIf
\State{$i=i-1$}
\EndWhile
\end{algorithmic}
\end{algorithm}

An example of Knapsack problem is show below. There are 4 items, and $W = 6$. 
\begin{center}
\begin{tabular}{c|cccc}
item  & 1 & 2 & 3 & 4\\\hline
$v_i$ & 3 & 2 & 4 & 4\\
$w_i$ & 4 & 3 & 2 & 3\\
\end{tabular}
\begin{tabular}{c|ccccc}
$A[i,x$]& $i=0$ & $i=1$ & $i=2$ & $i=3$ & $i=4$\\\hline
$x=0$   &	0   & 0     & 0     & 0     & 0\\
$x=1$   &	0   & 0     & 0     & 0     & 0\\
$x=2$   &	0   & 0     & 0     & 4     & 4\\
$x=3$   &	0   & 0     & 2     & 4     & 4\\
$x=4$   &	0   & 3     & 3     & 4     & 4\\
$x=5$   &	0   & 3     & 3     & 6     & 8\\
$x=6$   &	0   & 3     & 3     & 7     & 8\\
\end{tabular}
\end{center}

The maximum value is therefore 8, corresponding to the subset \{3,4\}.
\section{Sequence Alignment}
\begin{description}
\Input{String $X=x_1x_2\dots x_m$, $Y=y_1y_2\dots y_m$ over some alphabet $\Sigma$. Penalty $\alpha_{ab}$ for aligning $a$ with $b$, and $\alpha_{gap}$ for inserting a gap. Presumably $\alpha_{aa}=0,\forall a\in\Sigma$.}
\Output{An alignment of $X$ and $Y$ with minimum total penalty.}
\end{description}
Consider the last position of the alignment. There are 3 possible cases: $x_m\:\&\:y_n$, $x_m\:\&\:gap$, or $gap\:\&\:y_n$. Let $X'=X-x_m$ and $Y'=Y-y_j$. If the optimal alignment falls into the first case, then it can be proved by contradictory that it is the optimal alignment of $X'$ and $Y'$ plus aligning $x_m$ with $y_n$. Similar reasoning can be made for the other 2 cases. In general, let $X_i$ represent the first $i$ letters of $X$ and $Y_j$ represent the first $j$ letters of $Y$. Let $P_{ij}$ represent the optimal penalty for aligning $X_i$ and $Y_j$. Then we must have
\begin{equation*}
P_{ij}=\min\begin{cases}
\alpha_{x_iy_j}+P_{i-1,j-1}\\
\alpha_{gap}+P_{i,j-1}\\
\alpha_{gap}+P_{i-1,j}\\
\end{cases}\end{equation*}
As for the base cases, obviously we have $P_{0i}=P_{i0}=i\cdot\alpha_{gap}$. Now we hear the knock at the door of a DP algorithm, as shown in Algorithm \ref{alignmentdp}. Its running time is $O(mn)$.
\begin{algorithm}[ht]
\caption{Sequence Alignment(DP)}\label{alignmentdp}
\begin{algorithmic}[1]
\Input{Two strings $X,Y$ as stated above.}
\Output{$(m+1)\times(n+1)$ 2D array $A$ with $A[i][j]=P_{ij}$.}
\State{Initialize $A[i][0]=A[0][i]=i\cdot\alpha_{gap}$ for all $i$.}
\For{$i=1$ \textbf{to} $m$}
\For{$j=1$ \textbf{to} $n$}
\State{$A[i][j]=\min\{A[i-1][j-1]+\alpha_{ij},A[i][j-1]+\alpha_{gap},A[i-1][j]+\alpha_{gap}\}$}
\EndFor\EndFor
\end{algorithmic}
\end{algorithm}

Just like before, the actual solution can be reconstructed based on $A$, as shown in Algorithm \ref{alignmentreconstruction}. The running time is $O(m+n)$.
\begin{algorithm}[ht]
\caption{Sequence Alignment Reconstruction}\label{alignmentreconstruction}
\begin{algorithmic}[1]
\Input{Array $A$ computed in Algorithm \ref{alignmentdp}}.
\Output{The actual alignment}
\State{$i=m,j=n$}
\While{$i>0$ or $j>0$}
\If{$i==0$}
\State{Align all $j$ left characters in $Y$ align with a gap and return}
\ElsIf{$j==0$}
\State{Align all $i$ left characters in $X$ align with a gap and return}
\ElsIf{$A[i][j]==A[i-1][j-1]+\alpha_{ij}$}
\State{Align $x_i$ with $y_j$}
\State{$i=i-1,j=j-1$}
\ElsIf{$A[i][j]==A[i][j-1]+\alpha_{gap}$}
\State{Align $y_j$ with a gap}
\State{$j=j-1$}
\Else\State{Align $x_i$ with a gap}
\State{$i=i-1$}
\EndIf\EndWhile
\end{algorithmic}
\end{algorithm}
\section{Optimal Binary Search Trees}
In a BST, the time consumption of searching for an item $i$ is $O(d_i)$, in which $d_i$ is the depth of $i$ in the BST. If the search for each item is equally likely to occur, the ideal BST should be balanced. Nonetheless, if we have knowledge of the likelihood for each item to be searched for, the optimal BST is not necessarily balanced. More frequently visited items should be put closer to the root. In Huffman code problem, we aimed at minimizing the average coding length; here, our target is to minimize the average search time.
\begin{description}
\Input{Frequencies $p_i$ for items $i=1,2,\dots,n$. }
\Output{An optimal BST that minimizes the average search time
$C(T)=\sum\limits_{i}p_i(d_i+1).$}
\end{description}
For Huffman coding, a bottom-up greedy algorithm efficiently solves the problem. But here either bottom-up or top-down greedy algorithm cannot guarantee an optimal solution.

Suppose the optimal BST $T$ has left child tree $T_1$, right child tree $T_2$ and root $r$. Items $\{1,\dots,r-1\}$ are contained in $T_1$, while $\{r+1,\dots,n\}$ lie in $T_2$. Then we have
\begin{align*}
C(T)&=\sum\limits_{i=1}^np_i(d_i+1)=p_r + \sum\limits_{i=1}^{r-1}p_i(d_i+1) + \sum\limits_{i=r+1}^{n}p_i(d_i+1)\\
&=p_r+\sum\limits_{i=1}^{r-1}p_i(d_{1i}+1 + 1) + \sum\limits_{i=r+1}^{n}p_i(d_{2i}+1 + 1)\\
&=p_r+C(T_1)+\sum\limits_{i=1}^{r-1}p_i + C(T_2)+\sum\limits_{i=r+1}^{n}p_i\\
&=C(T_1)+C(T_2)+\sum\limits_{i=1}^{n}p_i.
\end{align*}
Thus it can be proved by contradiction that $T_1$ must be optimal for $\{1,\dots,r-1\}$, and $T_2$ must be optimal for $\{r+1,\dots,n\}$. 
\begin{lemma}\textbf{Optimal Structure Lemma}
If $T$ is an optimal BST for the keys $\{1,\dots,n\}$ with root $r$, then its subtrees $T_1$ and $T_2$ must be optimal BSTs respectively for $\{1,\dots,r-1\}$ and $\{r+1,\dots,n\}$.
\end{lemma}
For $1\leq i\leq j\leq n$, let $C_{ij}$ represent the average search time of an optimal BST for items $\{i,\dots,j\}$. According to the optimal structure lemma, we can set up the recurrence relation:
\begin{equation*}
C_{ij}=\min\limits_{r=i}^j\left(C_{i,r-1}+C_{r+1,j}\right)+\sum\limits_{k=i}^jp_k,
\end{equation*}
which leads to the DP algorithm \ref{optimalbstdp} to solve the problem.
\begin{algorithm}[ht]
\caption{Optimal BST(DP)}\label{optimalbstdp}
\begin{algorithmic}[1]
\Input{Frequencies $p_i$ for items $i=1,2,\dots,n$. }
\Output{$n\times n$ 2D array $A$ with $A[i][j]$ representing optimal average search time for items $\{i,\dots,j\}$. }
\State{Initialize $A[i][i]=p_i$ for $i=1,\dots,n$.}\Comment{Base case: single node.}
\For{$s=0$ \textbf{to} $n-1$}\Comment{$A[i][j]=0$ if $i>j$ or $i,j$ out of bound. }
\For{$i=1$ \textbf{to} $n-s$}
\State{$A[i][i+s]=\min\limits_{r=i}^{i+s}\left(A[i][r-1]+A[r+1][i+s]\right)+\sum\limits_{k=i}^{i+s}p_k$}
\EndFor\EndFor
\end{algorithmic}
\end{algorithm}

In total there are $O(n^2)$ sub-problem, and each requires $O(j-i)$ time. Hence the overall running time is $O(n^3)$. Nonetheless it has been proven that an optimized version of the algorithm takes only $O(n^2)$ time.
\section{Bellman Ford Algorithm}
We've already introduced Dijkstra's algorithm to solve the shortest path problem when edge lengths are non-negative. Bellman Ford algorithm comes to our rescue when there exist edge lengths with negative lengths. Also, it provides a distributed alternative to Dijkstra's algorithm, which is needed to solve the Internet routing problem.
\begin{description}
\Input{Directed graph $G(V,E)$ with edge lengths $c_e$ for each $e\in E$ and source vertex $s\in V$. }
\Output{For every destination $v\in V$, compute the length of a shortest $s-v$ path.}
\end{description} 
We face a dilemma when it comes to negative cycles: if we take them into account in the search for shortest paths, a lot of vertices will end up with shortest paths with length $-\infty$. If we require that they should not be included in the paths, the problem becomes unsolved in polynomial time, i.e. NP hard. For the moment, we just assume that they do not exist in the graphs. Later we will introduce a criteria that detects negative cycles with little increase in the amount of workload. 
\subsection{Algorithm}
If there exists no negative cycles, then the shortest $s-v$ path for any vertex $v$ contains at most $n-1$ edges, because any path containing $n$ edges is sure to contain a cycle, and we can always get rid of the cycle, thus reducing the total cost, without breaking the reachability from $s$ to $v$. This inspires us of the definition of the subproblems in the dynamic programming paradigm: a shortest path from $s$ to $v$ with $i$ edges, in which $i=0,1,\dots,n-1$. We have the following lemma correct even for graphs with negative cycles.
\begin{lemma}
Let $G(V,E)$ be a directed graph with edge length $c_e$ for edge $e$ and source vertex $s$. For any vertex $v$, let $P$ represent the shortest path from $s$ to $v$ with at most $i$ paths. Then one of the two cases must be true.
\begin{description}
\item[case 1]If $P$ has $\leq(i-1)$ edges, then it is also a shortest $s-v$ path with $\leq(i-1)$ edges.
\item[case 2]If $P$ contains $i$ edges with $(w,v)$ being the last hop, then $P'=P-(w,v)$ must be the shortest $s-w$ path with $\leq(i-1)$ edges. 
\end{description}
\end{lemma}
The lemma can be easily proved by contradiction. It can serve as the recursion relation for our dynamic programming algorithm. The number of candidates for the solution to the subproblem for vertex $v$ of size $i$ , whose length we will denote as $L_{i,v}$, is $1+in-degree(v)$: solution to the subproblem with size $i-1$, and every incident edge $(w,v)$ + solution to the subproblem for $w$ of size $i-1$, i.e.
\begin{equation*}
L_{i,v} = \min\left\{L_{i-1,v}, \min\limits_{(w,v)\in E}\left(L_{i-1,w}+c_{wv}\right)\right\},\:\forall v\in V.
\end{equation*}
\begin{algorithm}[ht]
\caption{Bellman Ford Algorithm}\label{bellmanford}
\begin{algorithmic}[1]
\Input{Directed graph $G(V,E)$ with edge length $c_e$ for all $e$ and source vertex $s$.}
\Output{$n\times m$ 2D array $A$ with $A[i][v]=L_{i,v}$, in which $i=0,1,\dots,n-1$, $v\in V$.}
\State{Initialize $A[0][s]=0$ and $A[0][v]=+\infty$ for all $v\neq s$.}
\For{$i=1$ \textbf{to} $n-1$}
\For{each $v\in V$}
\State{$A[i][v]=\min\left\{A[i-1][v], \min\limits_{(w,v)\in E}\left(A[i-1][w]+c_{wv}\right)\right\}$}
\EndFor\EndFor
\end{algorithmic}
\end{algorithm}

As long as $G$ contains no negative cycle, $A[n-1][v]$ is guaranteed to be the length of the shortest $s-v$ path for any $v\in V$. The running time of Algorithm \ref{bellmanford} is $O(mn)$, because there are $n$ iterations in the outer loop, and each edge is examined exactly once in the inner loop. Note that if or some $j<n-1$, we have $A[j][v]=A[j-1][v]$ for all $v$, then later iterations will by no means modify the values, thus we can halt the algorithm and use $A[j][v]$ as the output. 
\subsection{Detect Negative Cycles}
In order to detect negative cycles, we will prove the following theorem.
\begin{theorem}
$G$ contains no negative cycle $\iff$ One more iteration in the Bellman Ford algorithm makes no modification to the values, i.e. we will have $A[n][v]=A[n-1][v]$ for all $v\in V$. 
\end{theorem} 
\begin{proof}
($\rightarrow$) Guaranteed by the correctness of Bellman Ford algorithm. An $s-v$ path with $n$ edges can never be the shortest in a graph containing no negative cycle, thus $A[n][v]$ will be filled with the same value as $A[n-1][v]$.

($\rightarrow$) Let $d(v)=A[n-1][v]=A[n][v]$. For any $(w,v)\in E$, we have 
$$A[n][v]\leq A[n-1][w]+c_{wv},$$
i.e. $d(v)-d(w)\leq c_{wv}$. Therefore for an arbitrary cycle $C$, its length
$$\sum\limits_{(w,v)\in C}c_{wv}\geq \sum\limits_{(w,v)\in C}(d(v)-d(w))=0.$$
Thus all cycles have non-negative total length.
\end{proof}

Hence, by running the extended Bellman Ford algorithm, i.e. running one more iteration after obtaining $A[n-1][v]$, we can detect whether negative cycles exist in the graph. This extra iteration does not lead to essential increase of the running time, which is still $O(mn)$. 
\subsection{Space Optimization}
The 2D array $A$ uses $O(n^2)$ space. If all we need is the length of the shortest path for each vertex, the space consumption can be reduced to $O(n)$, because we only need $A[i-1]$ to derive $A[i]$. However, if we hope to reconstruct the shortest paths themselves, it seems inevitable to store the whole 2D array.

Yet we can do better. We can set up another 2D array $B$, in which $B[i][v]$ is the predecessor of $v$ along the shortest $s-v$ path with at most $i$ edges, or NULL if such path doest not exist. Initially $B[0][v]$ is set to NULL for all $v$. In case 1 of Algorithm \ref{bellmanford}, we simply set $B[i][v]=B[i-1][v]$, while in case 2 we set $B[i][v]=w$. Finally with $B[n-1][v]$ at hand, we can reconstruct the shortest $s-v$ path by tracing back the predecessor pointers. Only linear space is needed to store the content of $A[i],A[i-1],B[i],B[i-1]$ in each iteration.

The correctness of this method can be verified as follow. Suppose the predecessor of $v$ is $w$, then the $s-w$ part of the shortest $s-v$ path must be the shortest $s-w$ path, otherwise we would be able to construct a shorter $s-v$ path. Hence the 3rd-to-last vertex along the shortest $s-v$ path is the 2nd-to-last along the shortest $s-w$ path, i.e. the predecessor of $w$. By tracing back the predecessor pointers, we finally obtain the shortest $s-v$ path.

Negative cycles can also be found with the help of the predecessor pointers. In each iteration, we can check for cycles within the predecessors, i.e. $B[i]$. If a negative cycle exists, it will be detected\footnote{Still not sure of precise correctness proof.}. 
\subsection{Internet Routing}
The Bellman Ford algorithm is intuitively a distributed algorithm suitable for the Internet routing problem. Yet needs to be adjusted for this specific use case. 

First we switch from a source driven approach to a destination driven one, i.e. all directions in the Bellman Ford algorithm are reversed. Every vertex $v$ stores the length of the shortest path from $v$ to each destination $t$ and the successor of $v$ along that path. Of course this does not mean that each machine has to know how to get to anywhere on the Internet. Thanks to the hierarchy structure of the Internet, each machine need to store only the path to some node above its own level in this hierarchy, e.g. the router of a district, or the gateway of a school, and the node will handle the rest of the routing. 

Second, asynchrony needs to be taken into account. Instead of a ``pull-based'' approach, in which each vertex checks the status of its neighbors proactively, we should switch to a ``push-based'' approach, in which each vertex notices its neighbors when its status changes. The algorithm will finally converge given that there exists no negative cycle, but it might take exponential time.

Third, connection failures need to be handled properly to avoid the ``counting to infinity'' problem. This is achieved in reality by making each vertex $v$ store the entire shortest path to destinations, which significantly causes more space consumption, but guarantees the robustness in case of failures, and also permits more sophisticated route selection, i.e. selection of intermediate stops.
\section{All-Pairs Shortest Paths}
\subsection{Problem Definition}
We've been dealing with shortest paths with a vertex is fixed as source. The all-pairs shortest path problem aims at obtaining the shortest path for any two vertices.
\begin{description}
\Input{Directed graph $G(V,E)$ with edge cost $c_e$ for each $e\in E$.}
\Output{Either compute the length of a shortest $u\rightarrow v$ path for all pairs of vertices $u,v$, or report the existence of a negative cycle.}
\end{description}
If there are no negative edges, we can solve the APSP problem by running Dijkstra's algorithm $n$ times, which takes $O(mn\log n)$ time. In a sparse graph, i.e. when $m=O(n)$, this is equivalent to $O(n^2\log n)$. In a dense graph, i.e. when $m=O(n^2)$, this is $O(n^3\log n)$. 

If negative edges are allowed, we have to instead run Bellman Ford algorithm $n$ times, thus we end up with time consumption $O(mn^2)$, which is equivalent to $O(n^3)$ for sparse graph and $O(n^4)$ for dense graph.
\subsection{Floyd Warshall Algorithm}
Floyd Warshall algorithm is a $O(n^3)$ algorithm that solves the APSP problem. It is as good as Bellman-Ford algorithm for sparse graphs, and better for dense graphs. Whether there exists an algorithm that solves the APSP problem for dense graphs significantly faster than $O(n^3)$ remains an open question.

In the context of F-W algorithm, the vertices are ordered arbitrarily, i.e. we have $V=\{1,2,\dots,n\}$. Let $V^{(k)}=\{1,2,\dots,k\}$. Suppose $G$ has no negative cycle, then we have the following lemma.
\begin{lemma}\textbf{Optimal Substructure Lemma}
For a specific pair of vertices $i,j$, let $P$ be a shortest $i-j$ path with all internal nodes, i.e. nodes along the path other than $i$ and $j$, in $V^{(k)}$. Then one of the two cases must be true.
\begin{description}
\item[case 1]If $k$ is not internal to $P$, then $P$ is a shortest $i-j$ path with all internal nodes in $V^{(k-1)}$.
\item[case 2]If $k$ is internal to $P$, then $P=P_1+P_2$, in which $P_1$ is the shortest $i-k$ path with all internal nodes in $V^{(k-1)}$ and $P_2$ is the shortest $k-j$ path with all internal nodes in $V^{(k-1)}$.
\end{description}
\end{lemma}
The proof already seems cliche to us. The lemma reveals the recurrence relation needed for the DP algorithm, as shown in Algorithm \ref{floydwarshall}.
\begin{algorithm}[ht]
\caption{Floyd Warshall's APSP Algorithm}\label{floydwarshall}
\begin{algorithmic}[1]
\Input{Directed graph $G(V,E)$ with edge length $c_e$ for all $e$, containing no negative cycle.}
\Output{$n\times n\times(n+1)$ 3D array $A$ with $A[i][j][k]$ representing the length of the shortest $i-j$ path with all internal nodes in $V^{k}$.}
\State{Initialize $A[i][j][0]=\begin{cases}
0&if\:i=j\\
c_{ij}&if\:(i,j)\in E\\
+\infty&if\:i\neq j\:and\:(i,j)\notin E
\end{cases}$}
\For{$k=1$ to $n$}
\For{$i=1$ to $n$}
\For{$j=1$ to $n$}
\State{$A[i][j][k]=\min\{A[i][j][k-1], A[i][k][k-1]+A[k][j][k-1]\}$}
\EndFor\EndFor\EndFor
\end{algorithmic}
\end{algorithm}

Each sub problem takes $O(1)$ time, thus the overall time consumption is $O(n^3)$. 

If there exists a negative cycle, then we will have $A[i][i]<0$ for at least one $i\in V$ at the end of the algorithm, which can be used for the detection of negative cycles.

In terms of reconstruction, we can save an $n\times n$ 2D array $B$ in which $B[i][j]$ represents the max label among internal nodes on a shortest $i-j$ path. In case 2 of algorithm \ref{floydwarshall}, we set $B[i][j]=k$.
\subsection{Johnson's Algorithm}
John's algorithm enables us to solve the APSP problem with 1 invocation of Bellman Ford and $n$ invocations of Dijkstra when there exists negative edges. 

It is not always possible to choose a source vertex that can reach any vertex in the graph. Thus we create a fake source vertex $s$ that has an edge of cost 0 to every vertex. We run Bellman Ford algorithm for $s$, and obtain $p_v$, the length of the shortest path from $s$ to each vertex $v$. Consider an edge $i\rightarrow j$. Since the optimal $s-i$ path + $i\rightarrow j$ forms an $s-j$ path, we must have $p_i+c_{ij}\geq p_j$, i.e. $$c'_{ij}=c_{ij}+p_i-p_j\geq 0.$$
If we substitute $c_{ij}$, the cost of edge $i\rightarrow j$ with $c'_{ij}$, then the new graph contains no negative edge, which makes it eligible for Dijkstra's algorithm. Consider a path $s-t$ with length $L_{st}$ in the original graph.  The length of the new $s-t$ path is 
$$L'_{st}=L_{st}+p_s-p_t. $$
The difference between $L_{st}$ and $L'_{st}$ is constant, thus a shortest path in the original graph remains shortest in the new graph.

As a summary, we have John's algorithm illustrated in Algorithm 
\begin{algorithm}[ht]
\caption{Johnson's APSP Algorithm}\label{apspjohnson}
\begin{algorithmic}[1]
\Input{Directed graph $G(V,E)$ with edge length $c_e$ for all $e$.}
\Output{Report a negative cycle or  }
\State{Form graph $G'$ by adding a new vertex $s$ and a new edge $(s,v)$ of length 0 for each $v\in V$. }\Comment{$O(n)$}
\State{Run Bellman Ford on $G'$ with source $s$. If a negative cycle is detected, halt and report. }\Comment{The negative cycle cannot contain $s$ because it has no ingoing edge. $O(mn)$}
\State{Define $p_v$ = length of shortest $s-v$ path in $G'$. }
\State{For each edge $(u,v)\in E$, define $c'_{uv}=c_{uv} + p_u-p_v$ }\Comment{$O(m)$}
\State{For each vertex $u\in G$, run Dijkstra in $G$ with source $u$ and edge length $\{c'_e\}$ to obtain the shortest $u-v$ path $d'(u,v)$ for each $v\in V$. }\Comment{$O(nm\log n)$}
\State{For each pair of $(u,v)$, return the shortest path length $d(u,v)=d'(u,v)+p_v-p_u$. }\Comment{$O(n^2)$}
\end{algorithmic}
\end{algorithm}

The overall running time is dominated by the Dijkstra invocations, i.e. $O(mn\log n)$. 
\ifx\PREAMBLE\undefined
\end{document}
\fi