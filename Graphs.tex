\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\chapter{Graph Algorithms}
\textbf{Graphs represent pairwise relationships amongst a set of objects}. The objects are called vertices or nodes. The relationships are called edges or arcs, each connecting a pair of vertices. An edge can be directed or undirected. The set of vertices and the set of edges are noted respectively as $V$ and $E$. Graph is a concept heavily used in reality. Road networks, the web, social networks, precedence constraints are all examples of graphs.

A connected graph composed of $n$ vertices with no parallel edges has at least $n-1$ and at most $n(n-1)/2$ edges. Let $m$ represent the number of edges. In most applications, $m$ is $\Omega(n)$ and $O(n^2)$. If $m$ is $O(n)$ or close to it, the graph is called a sparse graph, while if $m$ is closer to $O(n^2)$, it's called a dense graph. Yet their delimitation is not strictly clear. 
\section{Representation}
\subsection{Adjacent Matrix}
An undirected graph $G$ with $n$ vertices and no parallel edges can be represented by an $n\times n$ 0-1 matrix $A$. $A_{ij}=1$ when and only when $G$ has an $i-j$ edge. Variants of this representation can easily accommodate parallel edges, weighted edges: just let $A_{ij}$ represent the number of parallel edges or the weight of the edge. For directed graphs, $i\rightarrow j$ can be represented by $A_{ij}=1$ and $A_{ji}=-1$.

Adjacent matrix representation requires $\Theta(n^2)$ space. For a dense graph this is fine, but for a sparse graph it is wasteful. 
\subsection{Adjacent Lists}
The adjacent lists representation is composed of 4 ingredients:
\begin{itemize}
\item Array/List of vertices. $\Theta(n)$ space.
\item Array/List of edges. $\Theta(m)$ space.
\item Each edge points to its end points. $\Theta(m)$ space.
\item Each vertex points to edges incident on it. $\Theta(m)$ space.
\end{itemize}
Adjacent lists representation requires $\Theta(n+m)$ space. 

The choice between the two representations depends on the density of the graph and operations to take. We will mainly use adjacent lists in this chapter.
\section{Minimum Cut}
\subsection{Definition}
\begin{definition}
A cut of a graph ($V,E$) is a partition of $V$ into two non-empty sets $A$ and $B$.
\end{definition}
A graph with $n$ vertices has $2^n-2$ possible cuts. 
\begin{definition}
The crossing edges of a cut($A,B$) are those with 
\begin{itemize}
\item one endpoint in $A$ and the other in $B$, for undirected graphs;
\item tail in $A$ and head in $B$, for directed graphs.
\end{itemize}
\end{definition}
We will try to solve the minimum cut problem:
\begin{description}
\item[Input]An undirected graph $G=(V,E)$ in which parallel edges are allowed.
\item[Output]A cut $(A,B)$ with minimum number of crossing edges.
\end{description}
A lot of problems in reality can be reduced to a minimum cut problems:
\begin{itemize}
\item Identify weakness point of physical networks;
\item Community detection in social networks;
\item Image segmentation.
\end{itemize}
\subsection{Random Contraction Algorithm}
\begin{algorithm}[ht]
\caption{Random Contraction}\label{randomcontraction}
\begin{algorithmic}[1]
\Input\Statex{An undirected graph $G=(V,E)$ in which parallel edges are allowed.}
\Output\Statex{A cut $(A,B)$ with minimum number of crossing edges}
\While{There are more than 2 vertices left}
\State{Pick a remaining edge $(u,v)$ randomly}\label{randomselection}
\State{Merge(or contract) $u,v$ into a single vertex}
\State{Remove self-loops}
\EndWhile
\State{Return cut represented by 2 final vertices}
\end{algorithmic}
\end{algorithm}
\subsection{Probability of Correctness}
Algorithm \ref{randomcontraction} is not guaranteed to always return a minimum cut. We have to iterate the whole algorithm multiple times and choose the cut with minimum number of crossing edges obtained during the process. In order to estimate the number of iterations needed, we will calculate the probability that a specific minimum cut $(A,B)$ is returned in one iteration.

If one of the crossing edges of $(A,B)$ is selected in step \ref{randomselection}, the minimum cut cannot be returned. If there are $k$ crossing edges in $(A,B)$, the probability that a crossing edge is selected at the first iteration is $k/m$. The number of edges decrease by an indefinite number at each iteration, while the number of vertices decrease by exactly 1 each time. Thus it is preferable to express the probability in terms of $n$ instead of $m$.

Each vertex $v$ is related to a cut $({v},V-{v})$. The number of crossing edges of this cut is the number of edges incident on $v$, i.e. the degree of $v$. Considering the definition of minimum cut, this number must be larger than $k$. We also know the sum of the degrees of all vertices: $\sum\limits_{v}degree(v)=2m$. Thus we have $2m=\sum\limits_{v}degree(v)>kn$. Hence the probability that a crossing edge is not selected at the first iteration in step \ref{randomselection} is $$1-k/m>2/n.$$

At the second iteration, the same argument still holds. Let $m'$ represent the number of remaining edges after the first iteration. We have $2m'=\sum\limits_{v}degree(v)>k(n-1)$. Thus the probability that a crossing edge is selected at the 2nd iteration under the condition that no crossing edge was selected at the 1st iteration is $$1-k/m'>2/(n-1)$$.

The same argument continues further until the last iteration, i.e. the $(n-2)^{th}$ iteration. Finally, the probability that no crossing edge is selected in the whole process, thus resulting in the minimum cut $(A,B)$ is
\begin{align*}
P(A,B)&>\left(1-\frac{2}{n}\right)\left(1-\frac{2}{n-1}\right)\cdots\left(1-\frac{2}{n-(n-3)}\right)\\
&=\frac{(n-2)\cdot(n-3)\cdots 2\cdot 1}{n\cdot(n-1)\cdots 4\cdot 3}\\
&=\frac{2}{n(n-1)}>\frac{1}{n^2}.
\end{align*}
The probability seems small, but it already makes great advancement when compared with brute-force method, in which case the probability to obtain $(A,B)$ in an iteration is $\frac{1}{2^n}$.

After $N$ iterations, the probability that $(A,B)$ has not been found is 
\begin{equation*}
P(not\:found)<\left(1-\frac{1}{n^2}\right)^N\leq e^{-\frac{N}{n^2}}.
\end{equation*}
If $N=n^2$, the probability is smaller than $1/e$. If $N=n^2\ln n$, it is smaller than $1/n$. 
\ifx\PREAMBLE\undefined
\end{document}
\fi